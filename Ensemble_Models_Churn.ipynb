{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning is a machine learning technique that enhances accuracy and resilience in forecasting by merging predictions from multiple models. It aims to mitigate errors or biases that may exist in individual models by leveraging the collective intelligence of the ensemble.\n",
    "\n",
    "The underlying concept behind ensemble learning is to combine the outputs of diverse models to create a more precise prediction. By considering multiple perspectives and utilizing the strengths of different models, ensemble learning improves the overall performance of the learning system. This approach not only enhances accuracy but also provides resilience against uncertainties in the data. By effectively merging predictions from multiple models, ensemble learning has proven to be a powerful tool in various domains, offering more robust and reliable forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"data:image/png;base64,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\" width=450 height=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict the churn rate based on behaviour  of customers for a  telecom dataset using Ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Orange Telecom's Churn Dataset, which consists of cleaned customer activity data (features), along with a churn label specifying whether a customer canceled the subscription, will be used to develop predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Account length</th>\n",
       "      <th>Area code</th>\n",
       "      <th>International plan</th>\n",
       "      <th>Voice mail plan</th>\n",
       "      <th>Number vmail messages</th>\n",
       "      <th>Total day minutes</th>\n",
       "      <th>Total day calls</th>\n",
       "      <th>Total day charge</th>\n",
       "      <th>Total eve minutes</th>\n",
       "      <th>Total eve calls</th>\n",
       "      <th>Total eve charge</th>\n",
       "      <th>Total night minutes</th>\n",
       "      <th>Total night calls</th>\n",
       "      <th>Total night charge</th>\n",
       "      <th>Total intl minutes</th>\n",
       "      <th>Total intl calls</th>\n",
       "      <th>Total intl charge</th>\n",
       "      <th>Customer service calls</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LA</td>\n",
       "      <td>117</td>\n",
       "      <td>408</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>184.5</td>\n",
       "      <td>97</td>\n",
       "      <td>31.37</td>\n",
       "      <td>351.6</td>\n",
       "      <td>80</td>\n",
       "      <td>29.89</td>\n",
       "      <td>215.8</td>\n",
       "      <td>90</td>\n",
       "      <td>9.71</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IN</td>\n",
       "      <td>65</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>129.1</td>\n",
       "      <td>137</td>\n",
       "      <td>21.95</td>\n",
       "      <td>228.5</td>\n",
       "      <td>83</td>\n",
       "      <td>19.42</td>\n",
       "      <td>208.8</td>\n",
       "      <td>111</td>\n",
       "      <td>9.40</td>\n",
       "      <td>12.7</td>\n",
       "      <td>6</td>\n",
       "      <td>3.43</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NY</td>\n",
       "      <td>161</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>332.9</td>\n",
       "      <td>67</td>\n",
       "      <td>56.59</td>\n",
       "      <td>317.8</td>\n",
       "      <td>97</td>\n",
       "      <td>27.01</td>\n",
       "      <td>160.6</td>\n",
       "      <td>128</td>\n",
       "      <td>7.23</td>\n",
       "      <td>5.4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SC</td>\n",
       "      <td>111</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>110.4</td>\n",
       "      <td>103</td>\n",
       "      <td>18.77</td>\n",
       "      <td>137.3</td>\n",
       "      <td>102</td>\n",
       "      <td>11.67</td>\n",
       "      <td>189.6</td>\n",
       "      <td>105</td>\n",
       "      <td>8.53</td>\n",
       "      <td>7.7</td>\n",
       "      <td>6</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HI</td>\n",
       "      <td>49</td>\n",
       "      <td>510</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>119.3</td>\n",
       "      <td>117</td>\n",
       "      <td>20.28</td>\n",
       "      <td>215.1</td>\n",
       "      <td>109</td>\n",
       "      <td>18.28</td>\n",
       "      <td>178.7</td>\n",
       "      <td>90</td>\n",
       "      <td>8.04</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Account length  Area code International plan Voice mail plan  \\\n",
       "0    LA             117        408                 No              No   \n",
       "1    IN              65        415                 No              No   \n",
       "2    NY             161        415                 No              No   \n",
       "3    SC             111        415                 No              No   \n",
       "4    HI              49        510                 No              No   \n",
       "\n",
       "   Number vmail messages  Total day minutes  Total day calls  \\\n",
       "0                      0              184.5               97   \n",
       "1                      0              129.1              137   \n",
       "2                      0              332.9               67   \n",
       "3                      0              110.4              103   \n",
       "4                      0              119.3              117   \n",
       "\n",
       "   Total day charge  Total eve minutes  Total eve calls  Total eve charge  \\\n",
       "0             31.37              351.6               80             29.89   \n",
       "1             21.95              228.5               83             19.42   \n",
       "2             56.59              317.8               97             27.01   \n",
       "3             18.77              137.3              102             11.67   \n",
       "4             20.28              215.1              109             18.28   \n",
       "\n",
       "   Total night minutes  Total night calls  Total night charge  \\\n",
       "0                215.8                 90                9.71   \n",
       "1                208.8                111                9.40   \n",
       "2                160.6                128                7.23   \n",
       "3                189.6                105                8.53   \n",
       "4                178.7                 90                8.04   \n",
       "\n",
       "   Total intl minutes  Total intl calls  Total intl charge  \\\n",
       "0                 8.7                 4               2.35   \n",
       "1                12.7                 6               3.43   \n",
       "2                 5.4                 9               1.46   \n",
       "3                 7.7                 6               2.08   \n",
       "4                11.1                 1               3.00   \n",
       "\n",
       "   Customer service calls  Churn  \n",
       "0                       1  False  \n",
       "1                       4   True  \n",
       "2                       4   True  \n",
       "3                       2  False  \n",
       "4                       1  False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"C:/Users/Prath/OneDrive/Documents/Datasets/telecom_churn.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill for missing age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].fillna(df['Age'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine SibSp and Parch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, col in enumerate(['SibSp','Parch']):\n",
    "    plt.figure(i)\n",
    "    sns.catplot(x=col,y=\"Survived\",data=df,kind='point', aspect=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Family_cnt']=df['SibSp']+df['Parch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PassengerId','Parch','SibSp'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2         1       3                             Heikkinen, Miss. Laina   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4         0       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age            Ticket     Fare Cabin Embarked  Family_cnt  \n",
       "0    male  22.0         A/5 21171   7.2500   NaN        S           1  \n",
       "1  female  38.0          PC 17599  71.2833   C85        C           1  \n",
       "2  female  26.0  STON/O2. 3101282   7.9250   NaN        S           0  \n",
       "3  female  35.0            113803  53.1000  C123        S           1  \n",
       "4    male  35.0            373450   8.0500   NaN        S           0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in missing & create indicator for Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State                     0\n",
       "Account length            0\n",
       "Area code                 0\n",
       "International plan        0\n",
       "Voice mail plan           0\n",
       "Number vmail messages     0\n",
       "Total day minutes         0\n",
       "Total day calls           0\n",
       "Total day charge          0\n",
       "Total eve minutes         0\n",
       "Total eve calls           0\n",
       "Total eve charge          0\n",
       "Total night minutes       0\n",
       "Total night calls         0\n",
       "Total night charge        0\n",
       "Total intl minutes        0\n",
       "Total intl calls          0\n",
       "Total intl charge         0\n",
       "Customer service calls    0\n",
       "Churn                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ca4b0_row0_col0, #T_ca4b0_row0_col1, #T_ca4b0_row0_col2, #T_ca4b0_row0_col3, #T_ca4b0_row0_col4, #T_ca4b0_row0_col5, #T_ca4b0_row0_col6, #T_ca4b0_row0_col7, #T_ca4b0_row0_col8, #T_ca4b0_row0_col9, #T_ca4b0_row0_col10, #T_ca4b0_row0_col11, #T_ca4b0_row0_col12, #T_ca4b0_row0_col13, #T_ca4b0_row0_col14, #T_ca4b0_row0_col15 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row1_col0, #T_ca4b0_row5_col0 {\n",
       "  background-color: #d9e7f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col1 {\n",
       "  background-color: #3a8ac2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row1_col2, #T_ca4b0_row1_col11, #T_ca4b0_row1_col12, #T_ca4b0_row2_col5, #T_ca4b0_row4_col12, #T_ca4b0_row5_col11, #T_ca4b0_row6_col11, #T_ca4b0_row7_col15 {\n",
       "  background-color: #f5f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col3 {\n",
       "  background-color: #bfd8ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col4, #T_ca4b0_row1_col7, #T_ca4b0_row1_col10, #T_ca4b0_row5_col4, #T_ca4b0_row5_col7, #T_ca4b0_row5_col10 {\n",
       "  background-color: #d9e8f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col5, #T_ca4b0_row3_col4, #T_ca4b0_row5_col5, #T_ca4b0_row7_col8 {\n",
       "  background-color: #eef5fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col6, #T_ca4b0_row5_col6 {\n",
       "  background-color: #b4d3e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col8, #T_ca4b0_row5_col8, #T_ca4b0_row7_col11, #T_ca4b0_row7_col13 {\n",
       "  background-color: #f2f8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col9 {\n",
       "  background-color: #b7d4ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col13, #T_ca4b0_row1_col14, #T_ca4b0_row2_col8, #T_ca4b0_row2_col12, #T_ca4b0_row3_col5, #T_ca4b0_row3_col8, #T_ca4b0_row4_col13, #T_ca4b0_row5_col13, #T_ca4b0_row5_col14, #T_ca4b0_row6_col14, #T_ca4b0_row7_col14 {\n",
       "  background-color: #f6faff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row1_col15, #T_ca4b0_row2_col11, #T_ca4b0_row2_col13, #T_ca4b0_row2_col14, #T_ca4b0_row2_col15, #T_ca4b0_row3_col0, #T_ca4b0_row3_col2, #T_ca4b0_row3_col11, #T_ca4b0_row3_col12, #T_ca4b0_row3_col13, #T_ca4b0_row3_col14, #T_ca4b0_row3_col15, #T_ca4b0_row4_col2, #T_ca4b0_row4_col14, #T_ca4b0_row4_col15, #T_ca4b0_row5_col2, #T_ca4b0_row5_col15, #T_ca4b0_row6_col15 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row2_col0 {\n",
       "  background-color: #ebf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row2_col1, #T_ca4b0_row3_col10 {\n",
       "  background-color: #eaf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row2_col2, #T_ca4b0_row4_col8 {\n",
       "  background-color: #f3f8fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row2_col3, #T_ca4b0_row7_col5 {\n",
       "  background-color: #e7f0fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row2_col4, #T_ca4b0_row2_col7, #T_ca4b0_row2_col10, #T_ca4b0_row6_col2, #T_ca4b0_row6_col8, #T_ca4b0_row7_col12 {\n",
       "  background-color: #f2f7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row2_col6, #T_ca4b0_row2_col9, #T_ca4b0_row7_col2 {\n",
       "  background-color: #e8f1fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row3_col1, #T_ca4b0_row4_col1 {\n",
       "  background-color: #4695c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row3_col3, #T_ca4b0_row4_col5 {\n",
       "  background-color: #f0f6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row3_col6 {\n",
       "  background-color: #e9f2fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row3_col7, #T_ca4b0_row6_col5 {\n",
       "  background-color: #ecf4fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row3_col9 {\n",
       "  background-color: #f1f7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row4_col0 {\n",
       "  background-color: #e0ecf8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row4_col3 {\n",
       "  background-color: #ccdff1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row4_col4, #T_ca4b0_row4_col7, #T_ca4b0_row4_col10 {\n",
       "  background-color: #ddeaf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row4_col6 {\n",
       "  background-color: #c4daee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row4_col9, #T_ca4b0_row7_col7 {\n",
       "  background-color: #c6dbef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row4_col11, #T_ca4b0_row6_col13 {\n",
       "  background-color: #f5fafe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row5_col1, #T_ca4b0_row6_col1 {\n",
       "  background-color: #4292c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row5_col3 {\n",
       "  background-color: #c1d9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row5_col9 {\n",
       "  background-color: #b5d4e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row5_col12, #T_ca4b0_row6_col12 {\n",
       "  background-color: #f4f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row6_col0 {\n",
       "  background-color: #d1e2f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row6_col3 {\n",
       "  background-color: #add0e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row6_col4 {\n",
       "  background-color: #d5e5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row6_col6 {\n",
       "  background-color: #a5cde3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row6_col7, #T_ca4b0_row6_col10 {\n",
       "  background-color: #d6e5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row6_col9 {\n",
       "  background-color: #a8cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row7_col0 {\n",
       "  background-color: #a6cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row7_col1 {\n",
       "  background-color: #1e6db2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row7_col3 {\n",
       "  background-color: #6aaed6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row7_col4 {\n",
       "  background-color: #c7dbef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_ca4b0_row7_col6 {\n",
       "  background-color: #5da5d1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row7_col9 {\n",
       "  background-color: #5aa2cf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_ca4b0_row7_col10 {\n",
       "  background-color: #c2d9ee;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ca4b0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ca4b0_level0_col0\" class=\"col_heading level0 col0\" >Account length</th>\n",
       "      <th id=\"T_ca4b0_level0_col1\" class=\"col_heading level0 col1\" >Area code</th>\n",
       "      <th id=\"T_ca4b0_level0_col2\" class=\"col_heading level0 col2\" >Number vmail messages</th>\n",
       "      <th id=\"T_ca4b0_level0_col3\" class=\"col_heading level0 col3\" >Total day minutes</th>\n",
       "      <th id=\"T_ca4b0_level0_col4\" class=\"col_heading level0 col4\" >Total day calls</th>\n",
       "      <th id=\"T_ca4b0_level0_col5\" class=\"col_heading level0 col5\" >Total day charge</th>\n",
       "      <th id=\"T_ca4b0_level0_col6\" class=\"col_heading level0 col6\" >Total eve minutes</th>\n",
       "      <th id=\"T_ca4b0_level0_col7\" class=\"col_heading level0 col7\" >Total eve calls</th>\n",
       "      <th id=\"T_ca4b0_level0_col8\" class=\"col_heading level0 col8\" >Total eve charge</th>\n",
       "      <th id=\"T_ca4b0_level0_col9\" class=\"col_heading level0 col9\" >Total night minutes</th>\n",
       "      <th id=\"T_ca4b0_level0_col10\" class=\"col_heading level0 col10\" >Total night calls</th>\n",
       "      <th id=\"T_ca4b0_level0_col11\" class=\"col_heading level0 col11\" >Total night charge</th>\n",
       "      <th id=\"T_ca4b0_level0_col12\" class=\"col_heading level0 col12\" >Total intl minutes</th>\n",
       "      <th id=\"T_ca4b0_level0_col13\" class=\"col_heading level0 col13\" >Total intl calls</th>\n",
       "      <th id=\"T_ca4b0_level0_col14\" class=\"col_heading level0 col14\" >Total intl charge</th>\n",
       "      <th id=\"T_ca4b0_level0_col15\" class=\"col_heading level0 col15\" >Customer service calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_ca4b0_row0_col0\" class=\"data row0 col0\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col1\" class=\"data row0 col1\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col2\" class=\"data row0 col2\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col3\" class=\"data row0 col3\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col4\" class=\"data row0 col4\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col5\" class=\"data row0 col5\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col6\" class=\"data row0 col6\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col7\" class=\"data row0 col7\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col8\" class=\"data row0 col8\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col9\" class=\"data row0 col9\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col10\" class=\"data row0 col10\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col11\" class=\"data row0 col11\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col12\" class=\"data row0 col12\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col13\" class=\"data row0 col13\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col14\" class=\"data row0 col14\" >667.000000</td>\n",
       "      <td id=\"T_ca4b0_row0_col15\" class=\"data row0 col15\" >667.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_ca4b0_row1_col0\" class=\"data row1 col0\" >102.841079</td>\n",
       "      <td id=\"T_ca4b0_row1_col1\" class=\"data row1 col1\" >436.157421</td>\n",
       "      <td id=\"T_ca4b0_row1_col2\" class=\"data row1 col2\" >8.407796</td>\n",
       "      <td id=\"T_ca4b0_row1_col3\" class=\"data row1 col3\" >180.948126</td>\n",
       "      <td id=\"T_ca4b0_row1_col4\" class=\"data row1 col4\" >100.937031</td>\n",
       "      <td id=\"T_ca4b0_row1_col5\" class=\"data row1 col5\" >30.761769</td>\n",
       "      <td id=\"T_ca4b0_row1_col6\" class=\"data row1 col6\" >203.355322</td>\n",
       "      <td id=\"T_ca4b0_row1_col7\" class=\"data row1 col7\" >100.476762</td>\n",
       "      <td id=\"T_ca4b0_row1_col8\" class=\"data row1 col8\" >17.285262</td>\n",
       "      <td id=\"T_ca4b0_row1_col9\" class=\"data row1 col9\" >199.685307</td>\n",
       "      <td id=\"T_ca4b0_row1_col10\" class=\"data row1 col10\" >100.113943</td>\n",
       "      <td id=\"T_ca4b0_row1_col11\" class=\"data row1 col11\" >8.985907</td>\n",
       "      <td id=\"T_ca4b0_row1_col12\" class=\"data row1 col12\" >10.238381</td>\n",
       "      <td id=\"T_ca4b0_row1_col13\" class=\"data row1 col13\" >4.527736</td>\n",
       "      <td id=\"T_ca4b0_row1_col14\" class=\"data row1 col14\" >2.764948</td>\n",
       "      <td id=\"T_ca4b0_row1_col15\" class=\"data row1 col15\" >1.563718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_ca4b0_row2_col0\" class=\"data row2 col0\" >40.819480</td>\n",
       "      <td id=\"T_ca4b0_row2_col1\" class=\"data row2 col1\" >41.783305</td>\n",
       "      <td id=\"T_ca4b0_row2_col2\" class=\"data row2 col2\" >13.994480</td>\n",
       "      <td id=\"T_ca4b0_row2_col3\" class=\"data row2 col3\" >55.508628</td>\n",
       "      <td id=\"T_ca4b0_row2_col4\" class=\"data row2 col4\" >20.396790</td>\n",
       "      <td id=\"T_ca4b0_row2_col5\" class=\"data row2 col5\" >9.436463</td>\n",
       "      <td id=\"T_ca4b0_row2_col6\" class=\"data row2 col6\" >49.719268</td>\n",
       "      <td id=\"T_ca4b0_row2_col7\" class=\"data row2 col7\" >18.948262</td>\n",
       "      <td id=\"T_ca4b0_row2_col8\" class=\"data row2 col8\" >4.226160</td>\n",
       "      <td id=\"T_ca4b0_row2_col9\" class=\"data row2 col9\" >49.759931</td>\n",
       "      <td id=\"T_ca4b0_row2_col10\" class=\"data row2 col10\" >20.172505</td>\n",
       "      <td id=\"T_ca4b0_row2_col11\" class=\"data row2 col11\" >2.239429</td>\n",
       "      <td id=\"T_ca4b0_row2_col12\" class=\"data row2 col12\" >2.807850</td>\n",
       "      <td id=\"T_ca4b0_row2_col13\" class=\"data row2 col13\" >2.482442</td>\n",
       "      <td id=\"T_ca4b0_row2_col14\" class=\"data row2 col14\" >0.758167</td>\n",
       "      <td id=\"T_ca4b0_row2_col15\" class=\"data row2 col15\" >1.333357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_ca4b0_row3_col0\" class=\"data row3 col0\" >1.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col1\" class=\"data row3 col1\" >408.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col3\" class=\"data row3 col3\" >25.900000</td>\n",
       "      <td id=\"T_ca4b0_row3_col4\" class=\"data row3 col4\" >30.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col5\" class=\"data row3 col5\" >4.400000</td>\n",
       "      <td id=\"T_ca4b0_row3_col6\" class=\"data row3 col6\" >48.100000</td>\n",
       "      <td id=\"T_ca4b0_row3_col7\" class=\"data row3 col7\" >37.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col8\" class=\"data row3 col8\" >4.090000</td>\n",
       "      <td id=\"T_ca4b0_row3_col9\" class=\"data row3 col9\" >23.200000</td>\n",
       "      <td id=\"T_ca4b0_row3_col10\" class=\"data row3 col10\" >42.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col11\" class=\"data row3 col11\" >1.040000</td>\n",
       "      <td id=\"T_ca4b0_row3_col12\" class=\"data row3 col12\" >0.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col13\" class=\"data row3 col13\" >0.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col14\" class=\"data row3 col14\" >0.000000</td>\n",
       "      <td id=\"T_ca4b0_row3_col15\" class=\"data row3 col15\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_ca4b0_row4_col0\" class=\"data row4 col0\" >76.000000</td>\n",
       "      <td id=\"T_ca4b0_row4_col1\" class=\"data row4 col1\" >408.000000</td>\n",
       "      <td id=\"T_ca4b0_row4_col2\" class=\"data row4 col2\" >0.000000</td>\n",
       "      <td id=\"T_ca4b0_row4_col3\" class=\"data row4 col3\" >146.250000</td>\n",
       "      <td id=\"T_ca4b0_row4_col4\" class=\"data row4 col4\" >87.500000</td>\n",
       "      <td id=\"T_ca4b0_row4_col5\" class=\"data row4 col5\" >24.860000</td>\n",
       "      <td id=\"T_ca4b0_row4_col6\" class=\"data row4 col6\" >171.050000</td>\n",
       "      <td id=\"T_ca4b0_row4_col7\" class=\"data row4 col7\" >88.000000</td>\n",
       "      <td id=\"T_ca4b0_row4_col8\" class=\"data row4 col8\" >14.540000</td>\n",
       "      <td id=\"T_ca4b0_row4_col9\" class=\"data row4 col9\" >167.950000</td>\n",
       "      <td id=\"T_ca4b0_row4_col10\" class=\"data row4 col10\" >86.000000</td>\n",
       "      <td id=\"T_ca4b0_row4_col11\" class=\"data row4 col11\" >7.560000</td>\n",
       "      <td id=\"T_ca4b0_row4_col12\" class=\"data row4 col12\" >8.600000</td>\n",
       "      <td id=\"T_ca4b0_row4_col13\" class=\"data row4 col13\" >3.000000</td>\n",
       "      <td id=\"T_ca4b0_row4_col14\" class=\"data row4 col14\" >2.320000</td>\n",
       "      <td id=\"T_ca4b0_row4_col15\" class=\"data row4 col15\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_ca4b0_row5_col0\" class=\"data row5 col0\" >102.000000</td>\n",
       "      <td id=\"T_ca4b0_row5_col1\" class=\"data row5 col1\" >415.000000</td>\n",
       "      <td id=\"T_ca4b0_row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "      <td id=\"T_ca4b0_row5_col3\" class=\"data row5 col3\" >178.300000</td>\n",
       "      <td id=\"T_ca4b0_row5_col4\" class=\"data row5 col4\" >101.000000</td>\n",
       "      <td id=\"T_ca4b0_row5_col5\" class=\"data row5 col5\" >30.310000</td>\n",
       "      <td id=\"T_ca4b0_row5_col6\" class=\"data row5 col6\" >203.700000</td>\n",
       "      <td id=\"T_ca4b0_row5_col7\" class=\"data row5 col7\" >101.000000</td>\n",
       "      <td id=\"T_ca4b0_row5_col8\" class=\"data row5 col8\" >17.310000</td>\n",
       "      <td id=\"T_ca4b0_row5_col9\" class=\"data row5 col9\" >201.600000</td>\n",
       "      <td id=\"T_ca4b0_row5_col10\" class=\"data row5 col10\" >100.000000</td>\n",
       "      <td id=\"T_ca4b0_row5_col11\" class=\"data row5 col11\" >9.070000</td>\n",
       "      <td id=\"T_ca4b0_row5_col12\" class=\"data row5 col12\" >10.500000</td>\n",
       "      <td id=\"T_ca4b0_row5_col13\" class=\"data row5 col13\" >4.000000</td>\n",
       "      <td id=\"T_ca4b0_row5_col14\" class=\"data row5 col14\" >2.840000</td>\n",
       "      <td id=\"T_ca4b0_row5_col15\" class=\"data row5 col15\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_ca4b0_row6_col0\" class=\"data row6 col0\" >128.000000</td>\n",
       "      <td id=\"T_ca4b0_row6_col1\" class=\"data row6 col1\" >415.000000</td>\n",
       "      <td id=\"T_ca4b0_row6_col2\" class=\"data row6 col2\" >20.000000</td>\n",
       "      <td id=\"T_ca4b0_row6_col3\" class=\"data row6 col3\" >220.700000</td>\n",
       "      <td id=\"T_ca4b0_row6_col4\" class=\"data row6 col4\" >115.000000</td>\n",
       "      <td id=\"T_ca4b0_row6_col5\" class=\"data row6 col5\" >37.520000</td>\n",
       "      <td id=\"T_ca4b0_row6_col6\" class=\"data row6 col6\" >236.450000</td>\n",
       "      <td id=\"T_ca4b0_row6_col7\" class=\"data row6 col7\" >113.000000</td>\n",
       "      <td id=\"T_ca4b0_row6_col8\" class=\"data row6 col8\" >20.095000</td>\n",
       "      <td id=\"T_ca4b0_row6_col9\" class=\"data row6 col9\" >231.500000</td>\n",
       "      <td id=\"T_ca4b0_row6_col10\" class=\"data row6 col10\" >113.500000</td>\n",
       "      <td id=\"T_ca4b0_row6_col11\" class=\"data row6 col11\" >10.420000</td>\n",
       "      <td id=\"T_ca4b0_row6_col12\" class=\"data row6 col12\" >12.050000</td>\n",
       "      <td id=\"T_ca4b0_row6_col13\" class=\"data row6 col13\" >6.000000</td>\n",
       "      <td id=\"T_ca4b0_row6_col14\" class=\"data row6 col14\" >3.255000</td>\n",
       "      <td id=\"T_ca4b0_row6_col15\" class=\"data row6 col15\" >2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca4b0_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_ca4b0_row7_col0\" class=\"data row7 col0\" >232.000000</td>\n",
       "      <td id=\"T_ca4b0_row7_col1\" class=\"data row7 col1\" >510.000000</td>\n",
       "      <td id=\"T_ca4b0_row7_col2\" class=\"data row7 col2\" >51.000000</td>\n",
       "      <td id=\"T_ca4b0_row7_col3\" class=\"data row7 col3\" >334.300000</td>\n",
       "      <td id=\"T_ca4b0_row7_col4\" class=\"data row7 col4\" >165.000000</td>\n",
       "      <td id=\"T_ca4b0_row7_col5\" class=\"data row7 col5\" >56.830000</td>\n",
       "      <td id=\"T_ca4b0_row7_col6\" class=\"data row7 col6\" >361.800000</td>\n",
       "      <td id=\"T_ca4b0_row7_col7\" class=\"data row7 col7\" >168.000000</td>\n",
       "      <td id=\"T_ca4b0_row7_col8\" class=\"data row7 col8\" >30.750000</td>\n",
       "      <td id=\"T_ca4b0_row7_col9\" class=\"data row7 col9\" >367.700000</td>\n",
       "      <td id=\"T_ca4b0_row7_col10\" class=\"data row7 col10\" >175.000000</td>\n",
       "      <td id=\"T_ca4b0_row7_col11\" class=\"data row7 col11\" >16.550000</td>\n",
       "      <td id=\"T_ca4b0_row7_col12\" class=\"data row7 col12\" >18.300000</td>\n",
       "      <td id=\"T_ca4b0_row7_col13\" class=\"data row7 col13\" >18.000000</td>\n",
       "      <td id=\"T_ca4b0_row7_col14\" class=\"data row7 col14\" >4.940000</td>\n",
       "      <td id=\"T_ca4b0_row7_col15\" class=\"data row7 col15\" >8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c4c6d28490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe().style.background_gradient(cmap=\"Blues\",axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns in the dataset\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False, drop='first')  # Use drop='first' to avoid multicollinearity\n",
    "encoded_cols = encoder.fit_transform(df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with the encoded columns\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5ce3c_row0_col0, #T_5ce3c_row4_col4 {\n",
       "  background-color: #cadef0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col1 {\n",
       "  background-color: #1764ab;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ce3c_row0_col2, #T_5ce3c_row0_col15, #T_5ce3c_row0_col17, #T_5ce3c_row0_col18, #T_5ce3c_row0_col19, #T_5ce3c_row0_col20, #T_5ce3c_row0_col21, #T_5ce3c_row0_col22, #T_5ce3c_row0_col23, #T_5ce3c_row0_col24, #T_5ce3c_row0_col25, #T_5ce3c_row0_col26, #T_5ce3c_row0_col27, #T_5ce3c_row0_col28, #T_5ce3c_row0_col29, #T_5ce3c_row0_col30, #T_5ce3c_row0_col31, #T_5ce3c_row0_col32, #T_5ce3c_row0_col33, #T_5ce3c_row0_col34, #T_5ce3c_row0_col35, #T_5ce3c_row0_col36, #T_5ce3c_row0_col37, #T_5ce3c_row0_col38, #T_5ce3c_row0_col39, #T_5ce3c_row0_col40, #T_5ce3c_row0_col41, #T_5ce3c_row0_col42, #T_5ce3c_row0_col43, #T_5ce3c_row0_col44, #T_5ce3c_row0_col45, #T_5ce3c_row0_col46, #T_5ce3c_row0_col47, #T_5ce3c_row0_col48, #T_5ce3c_row0_col49, #T_5ce3c_row0_col50, #T_5ce3c_row0_col51, #T_5ce3c_row0_col52, #T_5ce3c_row0_col53, #T_5ce3c_row0_col54, #T_5ce3c_row0_col55, #T_5ce3c_row0_col56, #T_5ce3c_row0_col57, #T_5ce3c_row0_col58, #T_5ce3c_row0_col59, #T_5ce3c_row0_col60, #T_5ce3c_row0_col61, #T_5ce3c_row0_col62, #T_5ce3c_row0_col63, #T_5ce3c_row0_col64, #T_5ce3c_row0_col65, #T_5ce3c_row0_col66, #T_5ce3c_row0_col67, #T_5ce3c_row0_col68, #T_5ce3c_row1_col2, #T_5ce3c_row1_col17, #T_5ce3c_row1_col18, #T_5ce3c_row1_col19, #T_5ce3c_row1_col20, #T_5ce3c_row1_col21, #T_5ce3c_row1_col22, #T_5ce3c_row1_col23, #T_5ce3c_row1_col24, #T_5ce3c_row1_col25, #T_5ce3c_row1_col26, #T_5ce3c_row1_col27, #T_5ce3c_row1_col28, #T_5ce3c_row1_col29, #T_5ce3c_row1_col30, #T_5ce3c_row1_col31, #T_5ce3c_row1_col32, #T_5ce3c_row1_col33, #T_5ce3c_row1_col34, #T_5ce3c_row1_col35, #T_5ce3c_row1_col36, #T_5ce3c_row1_col37, #T_5ce3c_row1_col38, #T_5ce3c_row1_col39, #T_5ce3c_row1_col40, #T_5ce3c_row1_col41, #T_5ce3c_row1_col42, #T_5ce3c_row1_col43, #T_5ce3c_row1_col44, #T_5ce3c_row1_col45, #T_5ce3c_row1_col46, #T_5ce3c_row1_col47, #T_5ce3c_row1_col48, #T_5ce3c_row1_col49, #T_5ce3c_row1_col50, #T_5ce3c_row1_col51, #T_5ce3c_row1_col52, #T_5ce3c_row1_col53, #T_5ce3c_row1_col54, #T_5ce3c_row1_col55, #T_5ce3c_row1_col56, #T_5ce3c_row1_col57, #T_5ce3c_row1_col58, #T_5ce3c_row1_col59, #T_5ce3c_row1_col60, #T_5ce3c_row1_col61, #T_5ce3c_row1_col62, #T_5ce3c_row1_col63, #T_5ce3c_row1_col64, #T_5ce3c_row1_col65, #T_5ce3c_row1_col66, #T_5ce3c_row1_col67, #T_5ce3c_row1_col68, #T_5ce3c_row2_col2, #T_5ce3c_row2_col14, #T_5ce3c_row2_col17, #T_5ce3c_row2_col18, #T_5ce3c_row2_col19, #T_5ce3c_row2_col20, #T_5ce3c_row2_col21, #T_5ce3c_row2_col22, #T_5ce3c_row2_col23, #T_5ce3c_row2_col24, #T_5ce3c_row2_col25, #T_5ce3c_row2_col26, #T_5ce3c_row2_col27, #T_5ce3c_row2_col28, #T_5ce3c_row2_col29, #T_5ce3c_row2_col30, #T_5ce3c_row2_col31, #T_5ce3c_row2_col32, #T_5ce3c_row2_col33, #T_5ce3c_row2_col34, #T_5ce3c_row2_col35, #T_5ce3c_row2_col36, #T_5ce3c_row2_col37, #T_5ce3c_row2_col38, #T_5ce3c_row2_col39, #T_5ce3c_row2_col40, #T_5ce3c_row2_col41, #T_5ce3c_row2_col42, #T_5ce3c_row2_col43, #T_5ce3c_row2_col44, #T_5ce3c_row2_col45, #T_5ce3c_row2_col46, #T_5ce3c_row2_col47, #T_5ce3c_row2_col48, #T_5ce3c_row2_col49, #T_5ce3c_row2_col50, #T_5ce3c_row2_col51, #T_5ce3c_row2_col52, #T_5ce3c_row2_col53, #T_5ce3c_row2_col54, #T_5ce3c_row2_col55, #T_5ce3c_row2_col56, #T_5ce3c_row2_col57, #T_5ce3c_row2_col58, #T_5ce3c_row2_col59, #T_5ce3c_row2_col60, #T_5ce3c_row2_col61, #T_5ce3c_row2_col62, #T_5ce3c_row2_col63, #T_5ce3c_row2_col64, #T_5ce3c_row2_col65, #T_5ce3c_row2_col66, #T_5ce3c_row2_col67, #T_5ce3c_row2_col68, #T_5ce3c_row3_col2, #T_5ce3c_row3_col17, #T_5ce3c_row3_col18, #T_5ce3c_row3_col19, #T_5ce3c_row3_col20, #T_5ce3c_row3_col21, #T_5ce3c_row3_col22, #T_5ce3c_row3_col23, #T_5ce3c_row3_col24, #T_5ce3c_row3_col25, #T_5ce3c_row3_col26, #T_5ce3c_row3_col27, #T_5ce3c_row3_col28, #T_5ce3c_row3_col29, #T_5ce3c_row3_col30, #T_5ce3c_row3_col31, #T_5ce3c_row3_col32, #T_5ce3c_row3_col33, #T_5ce3c_row3_col34, #T_5ce3c_row3_col35, #T_5ce3c_row3_col36, #T_5ce3c_row3_col37, #T_5ce3c_row3_col38, #T_5ce3c_row3_col39, #T_5ce3c_row3_col40, #T_5ce3c_row3_col41, #T_5ce3c_row3_col42, #T_5ce3c_row3_col43, #T_5ce3c_row3_col44, #T_5ce3c_row3_col45, #T_5ce3c_row3_col46, #T_5ce3c_row3_col47, #T_5ce3c_row3_col48, #T_5ce3c_row3_col49, #T_5ce3c_row3_col50, #T_5ce3c_row3_col51, #T_5ce3c_row3_col52, #T_5ce3c_row3_col53, #T_5ce3c_row3_col54, #T_5ce3c_row3_col55, #T_5ce3c_row3_col56, #T_5ce3c_row3_col57, #T_5ce3c_row3_col58, #T_5ce3c_row3_col59, #T_5ce3c_row3_col60, #T_5ce3c_row3_col61, #T_5ce3c_row3_col62, #T_5ce3c_row3_col63, #T_5ce3c_row3_col64, #T_5ce3c_row3_col65, #T_5ce3c_row3_col66, #T_5ce3c_row3_col67, #T_5ce3c_row3_col68, #T_5ce3c_row4_col2, #T_5ce3c_row4_col13, #T_5ce3c_row4_col15, #T_5ce3c_row4_col17, #T_5ce3c_row4_col18, #T_5ce3c_row4_col19, #T_5ce3c_row4_col20, #T_5ce3c_row4_col21, #T_5ce3c_row4_col22, #T_5ce3c_row4_col23, #T_5ce3c_row4_col24, #T_5ce3c_row4_col25, #T_5ce3c_row4_col26, #T_5ce3c_row4_col27, #T_5ce3c_row4_col28, #T_5ce3c_row4_col29, #T_5ce3c_row4_col30, #T_5ce3c_row4_col31, #T_5ce3c_row4_col32, #T_5ce3c_row4_col33, #T_5ce3c_row4_col34, #T_5ce3c_row4_col35, #T_5ce3c_row4_col36, #T_5ce3c_row4_col37, #T_5ce3c_row4_col38, #T_5ce3c_row4_col39, #T_5ce3c_row4_col40, #T_5ce3c_row4_col41, #T_5ce3c_row4_col42, #T_5ce3c_row4_col43, #T_5ce3c_row4_col44, #T_5ce3c_row4_col45, #T_5ce3c_row4_col46, #T_5ce3c_row4_col47, #T_5ce3c_row4_col48, #T_5ce3c_row4_col49, #T_5ce3c_row4_col50, #T_5ce3c_row4_col51, #T_5ce3c_row4_col52, #T_5ce3c_row4_col53, #T_5ce3c_row4_col54, #T_5ce3c_row4_col55, #T_5ce3c_row4_col56, #T_5ce3c_row4_col57, #T_5ce3c_row4_col58, #T_5ce3c_row4_col59, #T_5ce3c_row4_col60, #T_5ce3c_row4_col61, #T_5ce3c_row4_col62, #T_5ce3c_row4_col63, #T_5ce3c_row4_col64, #T_5ce3c_row4_col65, #T_5ce3c_row4_col66, #T_5ce3c_row4_col67, #T_5ce3c_row4_col68 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col3 {\n",
       "  background-color: #a3cce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col4, #T_5ce3c_row2_col7 {\n",
       "  background-color: #d2e3f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col5, #T_5ce3c_row0_col8 {\n",
       "  background-color: #ebf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col6 {\n",
       "  background-color: #3181bd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ce3c_row0_col7 {\n",
       "  background-color: #d8e7f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col9 {\n",
       "  background-color: #8abfdd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col10, #T_5ce3c_row4_col10 {\n",
       "  background-color: #d4e4f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col11, #T_5ce3c_row0_col12, #T_5ce3c_row1_col11, #T_5ce3c_row2_col13, #T_5ce3c_row3_col11, #T_5ce3c_row4_col11 {\n",
       "  background-color: #f4f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col13, #T_5ce3c_row1_col15, #T_5ce3c_row2_col12, #T_5ce3c_row2_col15 {\n",
       "  background-color: #f5fafe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row0_col14, #T_5ce3c_row1_col14, #T_5ce3c_row3_col14, #T_5ce3c_row3_col15, #T_5ce3c_row4_col14 {\n",
       "  background-color: #f6faff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col0 {\n",
       "  background-color: #deebf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col1, #T_5ce3c_row2_col1, #T_5ce3c_row3_col1 {\n",
       "  background-color: #1460a8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ce3c_row1_col3, #T_5ce3c_row2_col10 {\n",
       "  background-color: #c6dbef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col4, #T_5ce3c_row3_col6 {\n",
       "  background-color: #c1d9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col5 {\n",
       "  background-color: #eef5fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col6 {\n",
       "  background-color: #81badb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col7 {\n",
       "  background-color: #d7e6f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col8, #T_5ce3c_row3_col5, #T_5ce3c_row4_col8 {\n",
       "  background-color: #f0f6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col9 {\n",
       "  background-color: #91c3de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col10, #T_5ce3c_row3_col0, #T_5ce3c_row3_col3 {\n",
       "  background-color: #cddff1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col12 {\n",
       "  background-color: #f2f8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row1_col13, #T_5ce3c_row2_col11, #T_5ce3c_row3_col12, #T_5ce3c_row3_col13 {\n",
       "  background-color: #f5f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row2_col0, #T_5ce3c_row2_col9 {\n",
       "  background-color: #b2d2e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row2_col3 {\n",
       "  background-color: #3a8ac2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ce3c_row2_col4 {\n",
       "  background-color: #ddeaf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row2_col5 {\n",
       "  background-color: #e1edf8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row2_col6 {\n",
       "  background-color: #4292c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ce3c_row2_col8 {\n",
       "  background-color: #edf4fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row3_col4, #T_5ce3c_row3_col7 {\n",
       "  background-color: #d0e1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row3_col8, #T_5ce3c_row4_col12 {\n",
       "  background-color: #f3f8fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row3_col9 {\n",
       "  background-color: #9fcae1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row3_col10 {\n",
       "  background-color: #cfe1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row4_col0 {\n",
       "  background-color: #e4eff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row4_col1 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ce3c_row4_col3 {\n",
       "  background-color: #caddf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row4_col5 {\n",
       "  background-color: #eff6fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row4_col6 {\n",
       "  background-color: #8cc0dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row4_col7 {\n",
       "  background-color: #cde0f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ce3c_row4_col9 {\n",
       "  background-color: #a6cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5ce3c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5ce3c_level0_col0\" class=\"col_heading level0 col0\" >Account length</th>\n",
       "      <th id=\"T_5ce3c_level0_col1\" class=\"col_heading level0 col1\" >Area code</th>\n",
       "      <th id=\"T_5ce3c_level0_col2\" class=\"col_heading level0 col2\" >Number vmail messages</th>\n",
       "      <th id=\"T_5ce3c_level0_col3\" class=\"col_heading level0 col3\" >Total day minutes</th>\n",
       "      <th id=\"T_5ce3c_level0_col4\" class=\"col_heading level0 col4\" >Total day calls</th>\n",
       "      <th id=\"T_5ce3c_level0_col5\" class=\"col_heading level0 col5\" >Total day charge</th>\n",
       "      <th id=\"T_5ce3c_level0_col6\" class=\"col_heading level0 col6\" >Total eve minutes</th>\n",
       "      <th id=\"T_5ce3c_level0_col7\" class=\"col_heading level0 col7\" >Total eve calls</th>\n",
       "      <th id=\"T_5ce3c_level0_col8\" class=\"col_heading level0 col8\" >Total eve charge</th>\n",
       "      <th id=\"T_5ce3c_level0_col9\" class=\"col_heading level0 col9\" >Total night minutes</th>\n",
       "      <th id=\"T_5ce3c_level0_col10\" class=\"col_heading level0 col10\" >Total night calls</th>\n",
       "      <th id=\"T_5ce3c_level0_col11\" class=\"col_heading level0 col11\" >Total night charge</th>\n",
       "      <th id=\"T_5ce3c_level0_col12\" class=\"col_heading level0 col12\" >Total intl minutes</th>\n",
       "      <th id=\"T_5ce3c_level0_col13\" class=\"col_heading level0 col13\" >Total intl calls</th>\n",
       "      <th id=\"T_5ce3c_level0_col14\" class=\"col_heading level0 col14\" >Total intl charge</th>\n",
       "      <th id=\"T_5ce3c_level0_col15\" class=\"col_heading level0 col15\" >Customer service calls</th>\n",
       "      <th id=\"T_5ce3c_level0_col16\" class=\"col_heading level0 col16\" >Churn</th>\n",
       "      <th id=\"T_5ce3c_level0_col17\" class=\"col_heading level0 col17\" >State_AL</th>\n",
       "      <th id=\"T_5ce3c_level0_col18\" class=\"col_heading level0 col18\" >State_AR</th>\n",
       "      <th id=\"T_5ce3c_level0_col19\" class=\"col_heading level0 col19\" >State_AZ</th>\n",
       "      <th id=\"T_5ce3c_level0_col20\" class=\"col_heading level0 col20\" >State_CA</th>\n",
       "      <th id=\"T_5ce3c_level0_col21\" class=\"col_heading level0 col21\" >State_CO</th>\n",
       "      <th id=\"T_5ce3c_level0_col22\" class=\"col_heading level0 col22\" >State_CT</th>\n",
       "      <th id=\"T_5ce3c_level0_col23\" class=\"col_heading level0 col23\" >State_DC</th>\n",
       "      <th id=\"T_5ce3c_level0_col24\" class=\"col_heading level0 col24\" >State_DE</th>\n",
       "      <th id=\"T_5ce3c_level0_col25\" class=\"col_heading level0 col25\" >State_FL</th>\n",
       "      <th id=\"T_5ce3c_level0_col26\" class=\"col_heading level0 col26\" >State_GA</th>\n",
       "      <th id=\"T_5ce3c_level0_col27\" class=\"col_heading level0 col27\" >State_HI</th>\n",
       "      <th id=\"T_5ce3c_level0_col28\" class=\"col_heading level0 col28\" >State_IA</th>\n",
       "      <th id=\"T_5ce3c_level0_col29\" class=\"col_heading level0 col29\" >State_ID</th>\n",
       "      <th id=\"T_5ce3c_level0_col30\" class=\"col_heading level0 col30\" >State_IL</th>\n",
       "      <th id=\"T_5ce3c_level0_col31\" class=\"col_heading level0 col31\" >State_IN</th>\n",
       "      <th id=\"T_5ce3c_level0_col32\" class=\"col_heading level0 col32\" >State_KS</th>\n",
       "      <th id=\"T_5ce3c_level0_col33\" class=\"col_heading level0 col33\" >State_KY</th>\n",
       "      <th id=\"T_5ce3c_level0_col34\" class=\"col_heading level0 col34\" >State_LA</th>\n",
       "      <th id=\"T_5ce3c_level0_col35\" class=\"col_heading level0 col35\" >State_MA</th>\n",
       "      <th id=\"T_5ce3c_level0_col36\" class=\"col_heading level0 col36\" >State_MD</th>\n",
       "      <th id=\"T_5ce3c_level0_col37\" class=\"col_heading level0 col37\" >State_ME</th>\n",
       "      <th id=\"T_5ce3c_level0_col38\" class=\"col_heading level0 col38\" >State_MI</th>\n",
       "      <th id=\"T_5ce3c_level0_col39\" class=\"col_heading level0 col39\" >State_MN</th>\n",
       "      <th id=\"T_5ce3c_level0_col40\" class=\"col_heading level0 col40\" >State_MO</th>\n",
       "      <th id=\"T_5ce3c_level0_col41\" class=\"col_heading level0 col41\" >State_MS</th>\n",
       "      <th id=\"T_5ce3c_level0_col42\" class=\"col_heading level0 col42\" >State_MT</th>\n",
       "      <th id=\"T_5ce3c_level0_col43\" class=\"col_heading level0 col43\" >State_NC</th>\n",
       "      <th id=\"T_5ce3c_level0_col44\" class=\"col_heading level0 col44\" >State_ND</th>\n",
       "      <th id=\"T_5ce3c_level0_col45\" class=\"col_heading level0 col45\" >State_NE</th>\n",
       "      <th id=\"T_5ce3c_level0_col46\" class=\"col_heading level0 col46\" >State_NH</th>\n",
       "      <th id=\"T_5ce3c_level0_col47\" class=\"col_heading level0 col47\" >State_NJ</th>\n",
       "      <th id=\"T_5ce3c_level0_col48\" class=\"col_heading level0 col48\" >State_NM</th>\n",
       "      <th id=\"T_5ce3c_level0_col49\" class=\"col_heading level0 col49\" >State_NV</th>\n",
       "      <th id=\"T_5ce3c_level0_col50\" class=\"col_heading level0 col50\" >State_NY</th>\n",
       "      <th id=\"T_5ce3c_level0_col51\" class=\"col_heading level0 col51\" >State_OH</th>\n",
       "      <th id=\"T_5ce3c_level0_col52\" class=\"col_heading level0 col52\" >State_OK</th>\n",
       "      <th id=\"T_5ce3c_level0_col53\" class=\"col_heading level0 col53\" >State_OR</th>\n",
       "      <th id=\"T_5ce3c_level0_col54\" class=\"col_heading level0 col54\" >State_PA</th>\n",
       "      <th id=\"T_5ce3c_level0_col55\" class=\"col_heading level0 col55\" >State_RI</th>\n",
       "      <th id=\"T_5ce3c_level0_col56\" class=\"col_heading level0 col56\" >State_SC</th>\n",
       "      <th id=\"T_5ce3c_level0_col57\" class=\"col_heading level0 col57\" >State_SD</th>\n",
       "      <th id=\"T_5ce3c_level0_col58\" class=\"col_heading level0 col58\" >State_TN</th>\n",
       "      <th id=\"T_5ce3c_level0_col59\" class=\"col_heading level0 col59\" >State_TX</th>\n",
       "      <th id=\"T_5ce3c_level0_col60\" class=\"col_heading level0 col60\" >State_UT</th>\n",
       "      <th id=\"T_5ce3c_level0_col61\" class=\"col_heading level0 col61\" >State_VA</th>\n",
       "      <th id=\"T_5ce3c_level0_col62\" class=\"col_heading level0 col62\" >State_VT</th>\n",
       "      <th id=\"T_5ce3c_level0_col63\" class=\"col_heading level0 col63\" >State_WA</th>\n",
       "      <th id=\"T_5ce3c_level0_col64\" class=\"col_heading level0 col64\" >State_WI</th>\n",
       "      <th id=\"T_5ce3c_level0_col65\" class=\"col_heading level0 col65\" >State_WV</th>\n",
       "      <th id=\"T_5ce3c_level0_col66\" class=\"col_heading level0 col66\" >State_WY</th>\n",
       "      <th id=\"T_5ce3c_level0_col67\" class=\"col_heading level0 col67\" >International plan_Yes</th>\n",
       "      <th id=\"T_5ce3c_level0_col68\" class=\"col_heading level0 col68\" >Voice mail plan_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5ce3c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5ce3c_row0_col0\" class=\"data row0 col0\" >117</td>\n",
       "      <td id=\"T_5ce3c_row0_col1\" class=\"data row0 col1\" >408</td>\n",
       "      <td id=\"T_5ce3c_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_5ce3c_row0_col3\" class=\"data row0 col3\" >184.500000</td>\n",
       "      <td id=\"T_5ce3c_row0_col4\" class=\"data row0 col4\" >97</td>\n",
       "      <td id=\"T_5ce3c_row0_col5\" class=\"data row0 col5\" >31.370000</td>\n",
       "      <td id=\"T_5ce3c_row0_col6\" class=\"data row0 col6\" >351.600000</td>\n",
       "      <td id=\"T_5ce3c_row0_col7\" class=\"data row0 col7\" >80</td>\n",
       "      <td id=\"T_5ce3c_row0_col8\" class=\"data row0 col8\" >29.890000</td>\n",
       "      <td id=\"T_5ce3c_row0_col9\" class=\"data row0 col9\" >215.800000</td>\n",
       "      <td id=\"T_5ce3c_row0_col10\" class=\"data row0 col10\" >90</td>\n",
       "      <td id=\"T_5ce3c_row0_col11\" class=\"data row0 col11\" >9.710000</td>\n",
       "      <td id=\"T_5ce3c_row0_col12\" class=\"data row0 col12\" >8.700000</td>\n",
       "      <td id=\"T_5ce3c_row0_col13\" class=\"data row0 col13\" >4</td>\n",
       "      <td id=\"T_5ce3c_row0_col14\" class=\"data row0 col14\" >2.350000</td>\n",
       "      <td id=\"T_5ce3c_row0_col15\" class=\"data row0 col15\" >1</td>\n",
       "      <td id=\"T_5ce3c_row0_col16\" class=\"data row0 col16\" >False</td>\n",
       "      <td id=\"T_5ce3c_row0_col17\" class=\"data row0 col17\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col18\" class=\"data row0 col18\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col19\" class=\"data row0 col19\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col20\" class=\"data row0 col20\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col21\" class=\"data row0 col21\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col22\" class=\"data row0 col22\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col23\" class=\"data row0 col23\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col24\" class=\"data row0 col24\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col25\" class=\"data row0 col25\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col26\" class=\"data row0 col26\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col27\" class=\"data row0 col27\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col28\" class=\"data row0 col28\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col29\" class=\"data row0 col29\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col30\" class=\"data row0 col30\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col31\" class=\"data row0 col31\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col32\" class=\"data row0 col32\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col33\" class=\"data row0 col33\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col34\" class=\"data row0 col34\" >1.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col35\" class=\"data row0 col35\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col36\" class=\"data row0 col36\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col37\" class=\"data row0 col37\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col38\" class=\"data row0 col38\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col39\" class=\"data row0 col39\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col40\" class=\"data row0 col40\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col41\" class=\"data row0 col41\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col42\" class=\"data row0 col42\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col43\" class=\"data row0 col43\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col44\" class=\"data row0 col44\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col45\" class=\"data row0 col45\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col46\" class=\"data row0 col46\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col47\" class=\"data row0 col47\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col48\" class=\"data row0 col48\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col49\" class=\"data row0 col49\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col50\" class=\"data row0 col50\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col51\" class=\"data row0 col51\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col52\" class=\"data row0 col52\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col53\" class=\"data row0 col53\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col54\" class=\"data row0 col54\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col55\" class=\"data row0 col55\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col56\" class=\"data row0 col56\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col57\" class=\"data row0 col57\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col58\" class=\"data row0 col58\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col59\" class=\"data row0 col59\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col60\" class=\"data row0 col60\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col61\" class=\"data row0 col61\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col62\" class=\"data row0 col62\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col63\" class=\"data row0 col63\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col64\" class=\"data row0 col64\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col65\" class=\"data row0 col65\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col66\" class=\"data row0 col66\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col67\" class=\"data row0 col67\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row0_col68\" class=\"data row0 col68\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ce3c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5ce3c_row1_col0\" class=\"data row1 col0\" >65</td>\n",
       "      <td id=\"T_5ce3c_row1_col1\" class=\"data row1 col1\" >415</td>\n",
       "      <td id=\"T_5ce3c_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_5ce3c_row1_col3\" class=\"data row1 col3\" >129.100000</td>\n",
       "      <td id=\"T_5ce3c_row1_col4\" class=\"data row1 col4\" >137</td>\n",
       "      <td id=\"T_5ce3c_row1_col5\" class=\"data row1 col5\" >21.950000</td>\n",
       "      <td id=\"T_5ce3c_row1_col6\" class=\"data row1 col6\" >228.500000</td>\n",
       "      <td id=\"T_5ce3c_row1_col7\" class=\"data row1 col7\" >83</td>\n",
       "      <td id=\"T_5ce3c_row1_col8\" class=\"data row1 col8\" >19.420000</td>\n",
       "      <td id=\"T_5ce3c_row1_col9\" class=\"data row1 col9\" >208.800000</td>\n",
       "      <td id=\"T_5ce3c_row1_col10\" class=\"data row1 col10\" >111</td>\n",
       "      <td id=\"T_5ce3c_row1_col11\" class=\"data row1 col11\" >9.400000</td>\n",
       "      <td id=\"T_5ce3c_row1_col12\" class=\"data row1 col12\" >12.700000</td>\n",
       "      <td id=\"T_5ce3c_row1_col13\" class=\"data row1 col13\" >6</td>\n",
       "      <td id=\"T_5ce3c_row1_col14\" class=\"data row1 col14\" >3.430000</td>\n",
       "      <td id=\"T_5ce3c_row1_col15\" class=\"data row1 col15\" >4</td>\n",
       "      <td id=\"T_5ce3c_row1_col16\" class=\"data row1 col16\" >True</td>\n",
       "      <td id=\"T_5ce3c_row1_col17\" class=\"data row1 col17\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col18\" class=\"data row1 col18\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col19\" class=\"data row1 col19\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col20\" class=\"data row1 col20\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col21\" class=\"data row1 col21\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col22\" class=\"data row1 col22\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col23\" class=\"data row1 col23\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col24\" class=\"data row1 col24\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col25\" class=\"data row1 col25\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col26\" class=\"data row1 col26\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col27\" class=\"data row1 col27\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col28\" class=\"data row1 col28\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col29\" class=\"data row1 col29\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col30\" class=\"data row1 col30\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col31\" class=\"data row1 col31\" >1.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col32\" class=\"data row1 col32\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col33\" class=\"data row1 col33\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col34\" class=\"data row1 col34\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col35\" class=\"data row1 col35\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col36\" class=\"data row1 col36\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col37\" class=\"data row1 col37\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col38\" class=\"data row1 col38\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col39\" class=\"data row1 col39\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col40\" class=\"data row1 col40\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col41\" class=\"data row1 col41\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col42\" class=\"data row1 col42\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col43\" class=\"data row1 col43\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col44\" class=\"data row1 col44\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col45\" class=\"data row1 col45\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col46\" class=\"data row1 col46\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col47\" class=\"data row1 col47\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col48\" class=\"data row1 col48\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col49\" class=\"data row1 col49\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col50\" class=\"data row1 col50\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col51\" class=\"data row1 col51\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col52\" class=\"data row1 col52\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col53\" class=\"data row1 col53\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col54\" class=\"data row1 col54\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col55\" class=\"data row1 col55\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col56\" class=\"data row1 col56\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col57\" class=\"data row1 col57\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col58\" class=\"data row1 col58\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col59\" class=\"data row1 col59\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col60\" class=\"data row1 col60\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col61\" class=\"data row1 col61\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col62\" class=\"data row1 col62\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col63\" class=\"data row1 col63\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col64\" class=\"data row1 col64\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col65\" class=\"data row1 col65\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col66\" class=\"data row1 col66\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col67\" class=\"data row1 col67\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row1_col68\" class=\"data row1 col68\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ce3c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5ce3c_row2_col0\" class=\"data row2 col0\" >161</td>\n",
       "      <td id=\"T_5ce3c_row2_col1\" class=\"data row2 col1\" >415</td>\n",
       "      <td id=\"T_5ce3c_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_5ce3c_row2_col3\" class=\"data row2 col3\" >332.900000</td>\n",
       "      <td id=\"T_5ce3c_row2_col4\" class=\"data row2 col4\" >67</td>\n",
       "      <td id=\"T_5ce3c_row2_col5\" class=\"data row2 col5\" >56.590000</td>\n",
       "      <td id=\"T_5ce3c_row2_col6\" class=\"data row2 col6\" >317.800000</td>\n",
       "      <td id=\"T_5ce3c_row2_col7\" class=\"data row2 col7\" >97</td>\n",
       "      <td id=\"T_5ce3c_row2_col8\" class=\"data row2 col8\" >27.010000</td>\n",
       "      <td id=\"T_5ce3c_row2_col9\" class=\"data row2 col9\" >160.600000</td>\n",
       "      <td id=\"T_5ce3c_row2_col10\" class=\"data row2 col10\" >128</td>\n",
       "      <td id=\"T_5ce3c_row2_col11\" class=\"data row2 col11\" >7.230000</td>\n",
       "      <td id=\"T_5ce3c_row2_col12\" class=\"data row2 col12\" >5.400000</td>\n",
       "      <td id=\"T_5ce3c_row2_col13\" class=\"data row2 col13\" >9</td>\n",
       "      <td id=\"T_5ce3c_row2_col14\" class=\"data row2 col14\" >1.460000</td>\n",
       "      <td id=\"T_5ce3c_row2_col15\" class=\"data row2 col15\" >4</td>\n",
       "      <td id=\"T_5ce3c_row2_col16\" class=\"data row2 col16\" >True</td>\n",
       "      <td id=\"T_5ce3c_row2_col17\" class=\"data row2 col17\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col18\" class=\"data row2 col18\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col19\" class=\"data row2 col19\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col20\" class=\"data row2 col20\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col21\" class=\"data row2 col21\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col22\" class=\"data row2 col22\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col23\" class=\"data row2 col23\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col24\" class=\"data row2 col24\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col25\" class=\"data row2 col25\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col26\" class=\"data row2 col26\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col27\" class=\"data row2 col27\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col28\" class=\"data row2 col28\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col29\" class=\"data row2 col29\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col30\" class=\"data row2 col30\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col31\" class=\"data row2 col31\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col32\" class=\"data row2 col32\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col33\" class=\"data row2 col33\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col34\" class=\"data row2 col34\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col35\" class=\"data row2 col35\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col36\" class=\"data row2 col36\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col37\" class=\"data row2 col37\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col38\" class=\"data row2 col38\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col39\" class=\"data row2 col39\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col40\" class=\"data row2 col40\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col41\" class=\"data row2 col41\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col42\" class=\"data row2 col42\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col43\" class=\"data row2 col43\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col44\" class=\"data row2 col44\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col45\" class=\"data row2 col45\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col46\" class=\"data row2 col46\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col47\" class=\"data row2 col47\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col48\" class=\"data row2 col48\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col49\" class=\"data row2 col49\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col50\" class=\"data row2 col50\" >1.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col51\" class=\"data row2 col51\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col52\" class=\"data row2 col52\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col53\" class=\"data row2 col53\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col54\" class=\"data row2 col54\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col55\" class=\"data row2 col55\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col56\" class=\"data row2 col56\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col57\" class=\"data row2 col57\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col58\" class=\"data row2 col58\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col59\" class=\"data row2 col59\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col60\" class=\"data row2 col60\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col61\" class=\"data row2 col61\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col62\" class=\"data row2 col62\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col63\" class=\"data row2 col63\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col64\" class=\"data row2 col64\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col65\" class=\"data row2 col65\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col66\" class=\"data row2 col66\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col67\" class=\"data row2 col67\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row2_col68\" class=\"data row2 col68\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ce3c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5ce3c_row3_col0\" class=\"data row3 col0\" >111</td>\n",
       "      <td id=\"T_5ce3c_row3_col1\" class=\"data row3 col1\" >415</td>\n",
       "      <td id=\"T_5ce3c_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_5ce3c_row3_col3\" class=\"data row3 col3\" >110.400000</td>\n",
       "      <td id=\"T_5ce3c_row3_col4\" class=\"data row3 col4\" >103</td>\n",
       "      <td id=\"T_5ce3c_row3_col5\" class=\"data row3 col5\" >18.770000</td>\n",
       "      <td id=\"T_5ce3c_row3_col6\" class=\"data row3 col6\" >137.300000</td>\n",
       "      <td id=\"T_5ce3c_row3_col7\" class=\"data row3 col7\" >102</td>\n",
       "      <td id=\"T_5ce3c_row3_col8\" class=\"data row3 col8\" >11.670000</td>\n",
       "      <td id=\"T_5ce3c_row3_col9\" class=\"data row3 col9\" >189.600000</td>\n",
       "      <td id=\"T_5ce3c_row3_col10\" class=\"data row3 col10\" >105</td>\n",
       "      <td id=\"T_5ce3c_row3_col11\" class=\"data row3 col11\" >8.530000</td>\n",
       "      <td id=\"T_5ce3c_row3_col12\" class=\"data row3 col12\" >7.700000</td>\n",
       "      <td id=\"T_5ce3c_row3_col13\" class=\"data row3 col13\" >6</td>\n",
       "      <td id=\"T_5ce3c_row3_col14\" class=\"data row3 col14\" >2.080000</td>\n",
       "      <td id=\"T_5ce3c_row3_col15\" class=\"data row3 col15\" >2</td>\n",
       "      <td id=\"T_5ce3c_row3_col16\" class=\"data row3 col16\" >False</td>\n",
       "      <td id=\"T_5ce3c_row3_col17\" class=\"data row3 col17\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col18\" class=\"data row3 col18\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col19\" class=\"data row3 col19\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col20\" class=\"data row3 col20\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col21\" class=\"data row3 col21\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col22\" class=\"data row3 col22\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col23\" class=\"data row3 col23\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col24\" class=\"data row3 col24\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col25\" class=\"data row3 col25\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col26\" class=\"data row3 col26\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col27\" class=\"data row3 col27\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col28\" class=\"data row3 col28\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col29\" class=\"data row3 col29\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col30\" class=\"data row3 col30\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col31\" class=\"data row3 col31\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col32\" class=\"data row3 col32\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col33\" class=\"data row3 col33\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col34\" class=\"data row3 col34\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col35\" class=\"data row3 col35\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col36\" class=\"data row3 col36\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col37\" class=\"data row3 col37\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col38\" class=\"data row3 col38\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col39\" class=\"data row3 col39\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col40\" class=\"data row3 col40\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col41\" class=\"data row3 col41\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col42\" class=\"data row3 col42\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col43\" class=\"data row3 col43\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col44\" class=\"data row3 col44\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col45\" class=\"data row3 col45\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col46\" class=\"data row3 col46\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col47\" class=\"data row3 col47\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col48\" class=\"data row3 col48\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col49\" class=\"data row3 col49\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col50\" class=\"data row3 col50\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col51\" class=\"data row3 col51\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col52\" class=\"data row3 col52\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col53\" class=\"data row3 col53\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col54\" class=\"data row3 col54\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col55\" class=\"data row3 col55\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col56\" class=\"data row3 col56\" >1.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col57\" class=\"data row3 col57\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col58\" class=\"data row3 col58\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col59\" class=\"data row3 col59\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col60\" class=\"data row3 col60\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col61\" class=\"data row3 col61\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col62\" class=\"data row3 col62\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col63\" class=\"data row3 col63\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col64\" class=\"data row3 col64\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col65\" class=\"data row3 col65\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col66\" class=\"data row3 col66\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col67\" class=\"data row3 col67\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row3_col68\" class=\"data row3 col68\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ce3c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5ce3c_row4_col0\" class=\"data row4 col0\" >49</td>\n",
       "      <td id=\"T_5ce3c_row4_col1\" class=\"data row4 col1\" >510</td>\n",
       "      <td id=\"T_5ce3c_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_5ce3c_row4_col3\" class=\"data row4 col3\" >119.300000</td>\n",
       "      <td id=\"T_5ce3c_row4_col4\" class=\"data row4 col4\" >117</td>\n",
       "      <td id=\"T_5ce3c_row4_col5\" class=\"data row4 col5\" >20.280000</td>\n",
       "      <td id=\"T_5ce3c_row4_col6\" class=\"data row4 col6\" >215.100000</td>\n",
       "      <td id=\"T_5ce3c_row4_col7\" class=\"data row4 col7\" >109</td>\n",
       "      <td id=\"T_5ce3c_row4_col8\" class=\"data row4 col8\" >18.280000</td>\n",
       "      <td id=\"T_5ce3c_row4_col9\" class=\"data row4 col9\" >178.700000</td>\n",
       "      <td id=\"T_5ce3c_row4_col10\" class=\"data row4 col10\" >90</td>\n",
       "      <td id=\"T_5ce3c_row4_col11\" class=\"data row4 col11\" >8.040000</td>\n",
       "      <td id=\"T_5ce3c_row4_col12\" class=\"data row4 col12\" >11.100000</td>\n",
       "      <td id=\"T_5ce3c_row4_col13\" class=\"data row4 col13\" >1</td>\n",
       "      <td id=\"T_5ce3c_row4_col14\" class=\"data row4 col14\" >3.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col15\" class=\"data row4 col15\" >1</td>\n",
       "      <td id=\"T_5ce3c_row4_col16\" class=\"data row4 col16\" >False</td>\n",
       "      <td id=\"T_5ce3c_row4_col17\" class=\"data row4 col17\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col18\" class=\"data row4 col18\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col19\" class=\"data row4 col19\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col20\" class=\"data row4 col20\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col21\" class=\"data row4 col21\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col22\" class=\"data row4 col22\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col23\" class=\"data row4 col23\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col24\" class=\"data row4 col24\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col25\" class=\"data row4 col25\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col26\" class=\"data row4 col26\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col27\" class=\"data row4 col27\" >1.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col28\" class=\"data row4 col28\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col29\" class=\"data row4 col29\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col30\" class=\"data row4 col30\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col31\" class=\"data row4 col31\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col32\" class=\"data row4 col32\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col33\" class=\"data row4 col33\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col34\" class=\"data row4 col34\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col35\" class=\"data row4 col35\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col36\" class=\"data row4 col36\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col37\" class=\"data row4 col37\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col38\" class=\"data row4 col38\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col39\" class=\"data row4 col39\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col40\" class=\"data row4 col40\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col41\" class=\"data row4 col41\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col42\" class=\"data row4 col42\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col43\" class=\"data row4 col43\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col44\" class=\"data row4 col44\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col45\" class=\"data row4 col45\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col46\" class=\"data row4 col46\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col47\" class=\"data row4 col47\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col48\" class=\"data row4 col48\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col49\" class=\"data row4 col49\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col50\" class=\"data row4 col50\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col51\" class=\"data row4 col51\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col52\" class=\"data row4 col52\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col53\" class=\"data row4 col53\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col54\" class=\"data row4 col54\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col55\" class=\"data row4 col55\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col56\" class=\"data row4 col56\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col57\" class=\"data row4 col57\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col58\" class=\"data row4 col58\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col59\" class=\"data row4 col59\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col60\" class=\"data row4 col60\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col61\" class=\"data row4 col61\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col62\" class=\"data row4 col62\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col63\" class=\"data row4 col63\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col64\" class=\"data row4 col64\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col65\" class=\"data row4 col65\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col66\" class=\"data row4 col66\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col67\" class=\"data row4 col67\" >0.000000</td>\n",
       "      <td id=\"T_5ce3c_row4_col68\" class=\"data row4 col68\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c4cbd48eb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "df.head().style.background_gradient(cmap=\"Blues\",axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split into train, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Churn', axis=1)\n",
    "labels = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state = 42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.2\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "for dataset in [y_train, y_val, y_test]:\n",
    "    print(round(len(dataset) / len(labels), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('C:/Users/Prath/OneDrive/Desktop/train_features.csv', index=False)\n",
    "X_val.to_csv('C:/Users/Prath/OneDrive/Desktop/val_features.csv', index=False)\n",
    "X_test.to_csv('C:/Users/Prath/OneDrive/Desktop/test_features.csv', index=False)\n",
    "\n",
    "y_train.to_csv('C:/Users/Prath/OneDrive/Desktop/train_labels.csv', index=False)\n",
    "y_val.to_csv('C:/Users/Prath/OneDrive/Desktop/val_labels.csv', index=False)\n",
    "y_test.to_csv('C:/Users/Prath/OneDrive/Desktop/test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble modelingBoosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added.  technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png\" width=450 height=250/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoostClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_features=pd.read_csv('C:/Users/Prath/OneDrive/Desktop/train_features.csv')\n",
    "tr_labels=pd.read_csv('C:/Users/Prath/OneDrive/Desktop/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.85 (+/-0.016) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.855 (+/-0.041) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.865 (+/-0.037) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.86 (+/-0.051) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.878 (+/-0.019) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.88 (+/-0.02) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.865 (+/-0.046) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.875 (+/-0.042) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.857 (+/-0.062) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.862 (+/-0.063) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.847 (+/-0.01) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.857 (+/-0.046) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.857 (+/-0.046) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.852 (+/-0.058) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.85 (+/-0.0) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.865 (+/-0.043) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.888 (+/-0.061) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.875 (+/-0.035) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.87 (+/-0.02) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.878 (+/-0.033) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.888 (+/-0.032) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.88 (+/-0.041) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.865 (+/-0.048) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.87 (+/-0.034) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.865 (+/-0.06) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.878 (+/-0.048) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.862 (+/-0.061) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.868 (+/-0.034) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.86 (+/-0.068) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.852 (+/-0.058) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.84 (+/-0.058) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.83 (+/-0.012) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.833 (+/-0.012) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.84 (+/-0.037) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.82 (+/-0.064) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.832 (+/-0.046) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.87 (+/-0.037) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.86 (+/-0.01) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.845 (+/-0.072) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.86 (+/-0.058) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.88 (+/-0.068) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.87 (+/-0.051) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.84 (+/-0.053) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.88 (+/-0.041) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.882 (+/-0.072) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.875 (+/-0.052) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.83 (+/-0.101) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.878 (+/-0.07) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.865 (+/-0.066) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.87 (+/-0.085) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.302 (+/-0.563) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.302 (+/-0.563) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.302 (+/-0.563) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.302 (+/-0.563) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.172 (+/-0.04) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.182 (+/-0.037) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.178 (+/-0.04) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.172 (+/-0.04) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.447 (+/-0.589) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.54 (+/-0.582) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.442 (+/-0.598) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.418 (+/-0.596) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.807 (+/-0.129) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.81 (+/-0.097) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.8 (+/-0.105) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.82 (+/-0.106) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.847 (+/-0.053) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.82 (+/-0.098) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.832 (+/-0.075) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.838 (+/-0.089) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.152 (+/-0.01) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.152 (+/-0.01) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.152 (+/-0.01) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.152 (+/-0.01) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.332 (+/-0.527) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.338 (+/-0.521) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.335 (+/-0.524) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.335 (+/-0.524) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.408 (+/-0.5) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.408 (+/-0.57) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.41 (+/-0.517) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.43 (+/-0.5) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.698 (+/-0.55) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.607 (+/-0.613) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.685 (+/-0.553) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.707 (+/-0.552) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.855 (+/-0.058) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.83 (+/-0.08) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.832 (+/-0.093) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.838 (+/-0.076) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [1, 3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "cv = GridSearchCV(gb, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=250)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out picked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Prath/OneDrive/Desktop/GB_model.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv.best_estimator_, 'C:/Users/Prath/OneDrive/Desktop/GB_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bagging is an assembling approach that tries to resolve overfitting for class or the regression problems. Bagging pursuits to improve the accuracy and overall performance of gadget mastering algorithms. It does this by taking random subsets of an original dataset, with substitute, and fits either a classifier (for classification) or regressor (for regression) to each subset. Bagging is also known as Bootstrap aggregating. It is an ensemble learning approach that enhances the overall performance and accuracy of the gadget for learning algorithms. It is miles used to address bias-variance alternate-off increases and decreases the variance of a prediction version. The Bagging avoids overfitting of data and is used for each regression and classification of the class, in particular for the decision tree algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://www.simplilearn.com/ice9/free_resources_article_thumb/Bagging.PNG\" width=450 height=250/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'max_depth': 16, 'n_estimators': 250}\n",
      "\n",
      "0.852 (+/-0.019) for {'max_depth': 4, 'n_estimators': 5}\n",
      "0.86 (+/-0.019) for {'max_depth': 4, 'n_estimators': 50}\n",
      "0.855 (+/-0.012) for {'max_depth': 4, 'n_estimators': 250}\n",
      "0.858 (+/-0.012) for {'max_depth': 4, 'n_estimators': 500}\n",
      "0.868 (+/-0.073) for {'max_depth': 8, 'n_estimators': 5}\n",
      "0.872 (+/-0.043) for {'max_depth': 8, 'n_estimators': 50}\n",
      "0.87 (+/-0.034) for {'max_depth': 8, 'n_estimators': 250}\n",
      "0.87 (+/-0.034) for {'max_depth': 8, 'n_estimators': 500}\n",
      "0.868 (+/-0.041) for {'max_depth': 16, 'n_estimators': 5}\n",
      "0.878 (+/-0.024) for {'max_depth': 16, 'n_estimators': 50}\n",
      "0.88 (+/-0.037) for {'max_depth': 16, 'n_estimators': 250}\n",
      "0.88 (+/-0.034) for {'max_depth': 16, 'n_estimators': 500}\n",
      "0.857 (+/-0.025) for {'max_depth': 32, 'n_estimators': 5}\n",
      "0.878 (+/-0.019) for {'max_depth': 32, 'n_estimators': 50}\n",
      "0.878 (+/-0.048) for {'max_depth': 32, 'n_estimators': 250}\n",
      "0.875 (+/-0.035) for {'max_depth': 32, 'n_estimators': 500}\n",
      "0.85 (+/-0.042) for {'max_depth': None, 'n_estimators': 5}\n",
      "0.87 (+/-0.025) for {'max_depth': None, 'n_estimators': 50}\n",
      "0.87 (+/-0.02) for {'max_depth': None, 'n_estimators': 250}\n",
      "0.878 (+/-0.046) for {'max_depth': None, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [4, 8, 16, 32, None],\n",
    "}\n",
    "cv = GridSearchCV(rf, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=16, n_estimators=250)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Prath/OneDrive/Desktop/RF_model.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv.best_estimator_, 'C:/Users/Prath/OneDrive/Desktop/RF_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is one of the popular ensemble modeling techniques in machine learning. Various weak learners are ensembled in a parallel manner in such a way that by combining them with Meta learners, we can predict better predictions for the future.\n",
    "\n",
    "This ensemble technique works by applying input of combined multiple weak learners' predictions and Meta learners so that a better output prediction model can be achieved.\n",
    "\n",
    "In stacking, an algorithm takes the outputs of sub-models as input and attempts to learn how to best combine the input predictions to make a better output prediction.\n",
    "\n",
    "Stacking is also known as a stacked generalization and is an extended form of the Model Averaging Ensemble technique in which all sub-models equally participate as per their performance weights and build a new model with better predictions. This new model is stacked up on top of the others; this is the reason why it is named stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://editor.analyticsvidhya.com/uploads/39725Stacking.png\" width=450 height=450/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': None,\n",
       " 'estimators': [('gb', GradientBoostingClassifier()),\n",
       "  ('rf', RandomForestClassifier())],\n",
       " 'final_estimator': None,\n",
       " 'n_jobs': None,\n",
       " 'passthrough': False,\n",
       " 'stack_method': 'auto',\n",
       " 'verbose': 0,\n",
       " 'gb': GradientBoostingClassifier(),\n",
       " 'rf': RandomForestClassifier(),\n",
       " 'gb__ccp_alpha': 0.0,\n",
       " 'gb__criterion': 'friedman_mse',\n",
       " 'gb__init': None,\n",
       " 'gb__learning_rate': 0.1,\n",
       " 'gb__loss': 'deviance',\n",
       " 'gb__max_depth': 3,\n",
       " 'gb__max_features': None,\n",
       " 'gb__max_leaf_nodes': None,\n",
       " 'gb__min_impurity_decrease': 0.0,\n",
       " 'gb__min_samples_leaf': 1,\n",
       " 'gb__min_samples_split': 2,\n",
       " 'gb__min_weight_fraction_leaf': 0.0,\n",
       " 'gb__n_estimators': 100,\n",
       " 'gb__n_iter_no_change': None,\n",
       " 'gb__random_state': None,\n",
       " 'gb__subsample': 1.0,\n",
       " 'gb__tol': 0.0001,\n",
       " 'gb__validation_fraction': 0.1,\n",
       " 'gb__verbose': 0,\n",
       " 'gb__warm_start': False,\n",
       " 'rf__bootstrap': True,\n",
       " 'rf__ccp_alpha': 0.0,\n",
       " 'rf__class_weight': None,\n",
       " 'rf__criterion': 'gini',\n",
       " 'rf__max_depth': None,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_leaf_nodes': None,\n",
       " 'rf__max_samples': None,\n",
       " 'rf__min_impurity_decrease': 0.0,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__min_weight_fraction_leaf': 0.0,\n",
       " 'rf__n_estimators': 100,\n",
       " 'rf__n_jobs': None,\n",
       " 'rf__oob_score': False,\n",
       " 'rf__random_state': None,\n",
       " 'rf__verbose': 0,\n",
       " 'rf__warm_start': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[('gb',GradientBoostingClassifier()),('rf',RandomForestClassifier())]\n",
    "StackingClassifier(estimators=estimators).get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': None,\n",
       " 'estimators': [('gb', GradientBoostingClassifier()),\n",
       "  ('rf', RandomForestClassifier())],\n",
       " 'final_estimator': None,\n",
       " 'n_jobs': None,\n",
       " 'passthrough': False,\n",
       " 'stack_method': 'auto',\n",
       " 'verbose': 0,\n",
       " 'gb': GradientBoostingClassifier(),\n",
       " 'rf': RandomForestClassifier(),\n",
       " 'gb__ccp_alpha': 0.0,\n",
       " 'gb__criterion': 'friedman_mse',\n",
       " 'gb__init': None,\n",
       " 'gb__learning_rate': 0.1,\n",
       " 'gb__loss': 'deviance',\n",
       " 'gb__max_depth': 3,\n",
       " 'gb__max_features': None,\n",
       " 'gb__max_leaf_nodes': None,\n",
       " 'gb__min_impurity_decrease': 0.0,\n",
       " 'gb__min_samples_leaf': 1,\n",
       " 'gb__min_samples_split': 2,\n",
       " 'gb__min_weight_fraction_leaf': 0.0,\n",
       " 'gb__n_estimators': 100,\n",
       " 'gb__n_iter_no_change': None,\n",
       " 'gb__random_state': None,\n",
       " 'gb__subsample': 1.0,\n",
       " 'gb__tol': 0.0001,\n",
       " 'gb__validation_fraction': 0.1,\n",
       " 'gb__verbose': 0,\n",
       " 'gb__warm_start': False,\n",
       " 'rf__bootstrap': True,\n",
       " 'rf__ccp_alpha': 0.0,\n",
       " 'rf__class_weight': None,\n",
       " 'rf__criterion': 'gini',\n",
       " 'rf__max_depth': None,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_leaf_nodes': None,\n",
       " 'rf__max_samples': None,\n",
       " 'rf__min_impurity_decrease': 0.0,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__min_weight_fraction_leaf': 0.0,\n",
       " 'rf__n_estimators': 100,\n",
       " 'rf__n_jobs': None,\n",
       " 'rf__oob_score': False,\n",
       " 'rf__random_state': None,\n",
       " 'rf__verbose': 0,\n",
       " 'rf__warm_start': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators=[('gb',GradientBoostingClassifier()),('rf',RandomForestClassifier())]\n",
    "sc=StackingClassifier(estimators=estimators)\n",
    "sc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Prath\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 100, 'passthrough': False, 'rf__n_estimators': 50}\n",
      "\n",
      "0.85 (+/-0.05) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 50, 'passthrough': True, 'rf__n_estimators': 50}\n",
      "0.838 (+/-0.042) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 50, 'passthrough': True, 'rf__n_estimators': 100}\n",
      "0.847 (+/-0.01) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 50, 'passthrough': False, 'rf__n_estimators': 50}\n",
      "0.847 (+/-0.01) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 50, 'passthrough': False, 'rf__n_estimators': 100}\n",
      "0.847 (+/-0.043) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 100, 'passthrough': True, 'rf__n_estimators': 50}\n",
      "0.85 (+/-0.032) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 100, 'passthrough': True, 'rf__n_estimators': 100}\n",
      "0.847 (+/-0.01) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 100, 'passthrough': False, 'rf__n_estimators': 50}\n",
      "0.847 (+/-0.01) for {'final_estimator': LogisticRegression(C=0.1), 'gb__n_estimators': 100, 'passthrough': False, 'rf__n_estimators': 100}\n",
      "0.85 (+/-0.035) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 50, 'passthrough': True, 'rf__n_estimators': 50}\n",
      "0.847 (+/-0.043) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 50, 'passthrough': True, 'rf__n_estimators': 100}\n",
      "0.87 (+/-0.034) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 50, 'passthrough': False, 'rf__n_estimators': 50}\n",
      "0.878 (+/-0.024) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 50, 'passthrough': False, 'rf__n_estimators': 100}\n",
      "0.845 (+/-0.044) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 100, 'passthrough': True, 'rf__n_estimators': 50}\n",
      "0.852 (+/-0.033) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 100, 'passthrough': True, 'rf__n_estimators': 100}\n",
      "0.872 (+/-0.04) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 100, 'passthrough': False, 'rf__n_estimators': 50}\n",
      "0.878 (+/-0.033) for {'final_estimator': LogisticRegression(C=1), 'gb__n_estimators': 100, 'passthrough': False, 'rf__n_estimators': 100}\n",
      "0.855 (+/-0.037) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 50, 'passthrough': True, 'rf__n_estimators': 50}\n",
      "0.852 (+/-0.033) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 50, 'passthrough': True, 'rf__n_estimators': 100}\n",
      "0.88 (+/-0.02) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 50, 'passthrough': False, 'rf__n_estimators': 50}\n",
      "0.878 (+/-0.037) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 50, 'passthrough': False, 'rf__n_estimators': 100}\n",
      "0.845 (+/-0.046) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 100, 'passthrough': True, 'rf__n_estimators': 50}\n",
      "0.847 (+/-0.043) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 100, 'passthrough': True, 'rf__n_estimators': 100}\n",
      "0.883 (+/-0.03) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 100, 'passthrough': False, 'rf__n_estimators': 50}\n",
      "0.883 (+/-0.034) for {'final_estimator': LogisticRegression(C=10), 'gb__n_estimators': 100, 'passthrough': False, 'rf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'gb__n_estimators': [50, 100],\n",
    "    'rf__n_estimators': [50, 100],\n",
    "    'final_estimator':[LogisticRegression(C=0.1),\n",
    "                      LogisticRegression(C=1),\n",
    "                      LogisticRegression(C=10)],\n",
    "    'passthrough':[True,False]\n",
    "}\n",
    "cv = GridSearchCV(sc, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Prath/OneDrive/Desktop/Stacked_model.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv.best_estimator_, 'C:/Users/Prath/OneDrive/Desktop/Stacked_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6. Comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features=pd.read_csv('C:/Users/Prath/OneDrive/Desktop/val_features.csv')\n",
    "val_labels=pd.read_csv('C:/Users/Prath/OneDrive/Desktop/val_labels.csv')\n",
    "\n",
    "te_features=pd.read_csv('C:/Users/Prath/OneDrive/Desktop/test_features.csv')\n",
    "te_labels=pd.read_csv('C:/Users/Prath/OneDrive/Desktop/test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mdl = joblib.load('C:/Users/Prath/OneDrive/Desktop/GB_model.pkl')\n",
    "rf_mdl = joblib.load('C:/Users/Prath/OneDrive/Desktop/RF_model.pkl')\n",
    "stacked_mdl = joblib.load('C:/Users/Prath/OneDrive/Desktop/Stacked_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(str(model).split('(')[0],\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier -- Accuracy: 0.94 / Precision: 0.647 / Recall: 0.846 / Latency: 3.3ms\n",
      "RandomForestClassifier -- Accuracy: 0.925 / Precision: 0.8 / Recall: 0.308 / Latency: 29.4ms\n",
      "StackingClassifier -- Accuracy: 0.932 / Precision: 0.667 / Recall: 0.615 / Latency: 9.0ms\n"
     ]
    }
   ],
   "source": [
    "for mdl in [gb_mdl,rf_mdl,stacked_mdl]:\n",
    "    evaluate_model(mdl,val_features, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier -- Accuracy: 0.94 / Precision: 0.882 / Recall: 0.714 / Latency: 3.0ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(gb_mdl, te_features, te_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have learnt about the ensemble models namely Boosting, Bagging and Stacking. We have built each of these models using the Titanic dataset and compared the accuracy, precision and Recall of the models. From the above, we can conclude that Gradient Boosting Classifier Model with 94% accuracy with validation set and 94% accuracy with the test set is the best model for our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
